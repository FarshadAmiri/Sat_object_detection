{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slicing_inference import sahi_slicing_inference\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from imageutils import resize_img\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 70 number of slices.\n",
      "Object found -> bbox:  [172.24466 219.75867 183.78247 252.8877 ]\n",
      "Object found -> bbox:  [178.79358 174.96414 215.41855 188.38754]\n",
      "Object found -> bbox:  [ 80.7796  184.60483  95.30228 237.50015]\n",
      "Object found -> bbox:  [ 56.39531 183.7208   66.48006 224.15482]\n",
      "Object found -> bbox:  [ 46.867878 190.34285   54.10033  219.8062  ]\n",
      "Object found -> bbox:  [ 87.29237  104.747406 126.65727  250.9613  ]\n",
      "Object found -> bbox:  [172.02037   14.113773 184.64522   49.890957]\n",
      "Object found -> bbox:  [56.044838 41.62107  67.34849  88.261986]\n",
      "Object found -> bbox:  [46.091427  0.       55.038563 15.20483 ]\n",
      "Object found -> bbox:  [81.16937     0.19804001 94.71823    30.920658  ]\n",
      "Object found -> bbox:  [80.39027 34.13742 88.7575  58.11094]\n",
      "Object found -> bbox:  [178.77655 109.04211 204.41302 139.4227 ]\n",
      "Object found -> bbox:  [ 72.8449   234.10031   93.480736 255.2612  ]\n",
      "Object found -> bbox:  [ 67.26007  87.34427 190.18393 215.46655]\n",
      "Object found -> bbox:  [73.17347  28.47319  96.60549  55.731316]\n",
      "Object found -> bbox:  [  5.4321938  96.69139   130.84628   209.5995   ]\n"
     ]
    }
   ],
   "source": [
    "image_path=r\"D:\\NLP 1\\Sat_object_detection\\debug_images\\2.jpg\"\n",
    "result = sahi_slicing_inference(image_path, scale_down_factor=4, confidence_threshold=0.85, model_input_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "tensor([[ 820.2710,  615.8837,  915.9689,  689.3981],\n",
      "        [ 605.2632,  342.0967,  630.9042,  443.5218],\n",
      "        [ 728.5158,  603.5172,  808.0978,  690.0929],\n",
      "        [1129.8649,  302.0988, 1137.3132,  332.0983],\n",
      "        [ 868.5435,  565.1304,  883.9681,  582.7642],\n",
      "        [ 802.4941,  253.8459,  826.7433,  261.3813],\n",
      "        [1145.9186,  257.6726, 1155.4994,  295.3320],\n",
      "        [ 836.2240,  222.5385, 1073.3400,  257.0076],\n",
      "        [ 797.9913,  283.8180,  806.2943,  304.2020]])\n",
      "tensor([0.9984, 0.9968, 0.9839, 0.9553, 0.9517, 0.9508, 0.9273, 0.9204, 0.8515])\n",
      "(1385, 918)\n"
     ]
    }
   ],
   "source": [
    "print(result['n_obj'])\n",
    "print(result['bboxes'])\n",
    "print(result['scores'])\n",
    "print(result[\"scaled_down_image_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['prediction'].export_visuals(export_dir=\"demo_data/\", hide_labels=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5513\n",
      "8313\n"
     ]
    }
   ],
   "source": [
    "image = np.asarray(Image.open(image_path))\n",
    "h, w, _ = image.shape\n",
    "print(h)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "image_path=r\"D:\\NLP 1\\Sat_object_detection\\debug_images\\2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "model = torch.load('models/best_model.pth', map_location = 'cuda')\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NLP 1\\venv\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "D:\\NLP 1\\venv\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from inference_modular import ship_detection_standard, ship_detection_sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ship_detection_standard(image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_obj': 2,\n",
       " 'bboxes': array([[402.41006, 461.42526, 438.3002 , 499.40314],\n",
       "        [452.47946, 246.16267, 613.4962 , 272.27393]], dtype=float32),\n",
       " 'scores': array([0.9397224 , 0.86912686], dtype=float32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAHI_SCALE_DOWN_FACTOR set to: 2\n",
      "Performing prediction on 35 number of slices.\n",
      "Object found -> bbox:  [365.84445 314.21194 422.8798  333.62717]\n",
      "Object found -> bbox:  [580.10266 404.423   665.64075 732.0508 ]\n",
      "Object found -> bbox:  [548.76794 232.16986 573.2628  310.2199 ]\n",
      "Object found -> bbox:  [564.43    146.43675 637.3945  170.1927 ]\n",
      "Object found -> bbox:  [2.5482941e-01 4.1782794e+02 4.3468842e+01 7.1294550e+02]\n",
      "Object found -> bbox:  [365.14713 162.96207 396.70053 268.37592]\n",
      "Object found -> bbox:  [316.6089  292.47156 342.76288 389.72583]\n",
      "Object found -> bbox:  [297.05743 173.99838 317.56848 234.91306]\n",
      "Object found -> bbox:  [363.67947 272.35434 382.48337 327.29276]\n",
      "Object found -> bbox:  [ 51.444565 161.20921   85.14     268.91467 ]\n",
      "Object found -> bbox:  [  3.8286781 292.93976    29.132156  389.64     ]\n",
      "Object found -> bbox:  [ 50.694202 274.86154   68.593956 326.1243  ]\n",
      "Object found -> bbox:  [559.8498  217.36679 618.7164  283.8817 ]\n",
      "Object found -> bbox:  [340.9956  588.1017  508.9184  764.06287]\n",
      "Object found -> bbox:  [144.0585  466.19012 195.90875 523.7467 ]\n",
      "Object found -> bbox:  [3.4738318e+02 7.0526123e-02 5.9599396e+02 2.1781741e+02]\n",
      "Object found -> bbox:  [8.9859009e-01 8.4381104e-03 2.7435013e+02 2.1220071e+02]\n"
     ]
    }
   ],
   "source": [
    "result = ship_detection_sahi(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_obj': 12,\n",
       " 'bboxes': array([[2604.0586 , 1696.1902 , 2655.9087 , 1753.7467 ],\n",
       "        [2404.8499 , 1447.3668 , 2463.7163 , 1513.8817 ],\n",
       "        [2393.768  ,  847.16986, 2418.2627 ,  925.2199 ],\n",
       "        [1810.1027 , 1019.423  , 1895.6407 , 1347.0508 ],\n",
       "        [3439.4446 ,  776.2092 , 3473.14   ,  883.9147 ],\n",
       "        [3391.609  ,  907.47156, 3417.763  , 1004.7258 ],\n",
       "        [ 365.84445,  314.21194,  422.8798 ,  333.62717],\n",
       "        [2409.43   ,  761.43677, 2482.3945 ,  785.1927 ],\n",
       "        [2460.8987 , 1845.0084 , 2734.35   , 2057.2007 ],\n",
       "        [2185.9956 , 1818.1017 , 2440.994  , 2062.8174 ],\n",
       "        [3372.0574 ,  788.9984 , 3392.5684 ,  849.9131 ],\n",
       "        [3438.6794 ,  887.3544 , 3457.4834 ,  942.2927 ]], dtype=float32),\n",
       " 'scores': array([0.99890566, 0.99799794, 0.9953642 , 0.99514717, 0.99324256,\n",
       "        0.98961747, 0.9888976 , 0.9886873 , 0.9868203 , 0.9807813 ,\n",
       "        0.96832097, 0.9658445 ], dtype=float32),\n",
       " 'sahi_scaled_down_image': Image([[[0.6588, 0.6863, 0.7451,  ..., 0.7294, 0.7176, 0.6784],\n",
       "         [0.6275, 0.6000, 0.6902,  ..., 0.6824, 0.7098, 0.7059],\n",
       "         [0.6118, 0.5843, 0.6745,  ..., 0.6510, 0.6667, 0.6784],\n",
       "         ...,\n",
       "         [0.1569, 0.1804, 0.1804,  ..., 0.1804, 0.1804, 0.1804],\n",
       "         [0.1725, 0.2000, 0.2000,  ..., 0.2000, 0.2000, 0.2000],\n",
       "         [0.1098, 0.1216, 0.1216,  ..., 0.1216, 0.1216, 0.1255]],\n",
       " \n",
       "        [[0.6706, 0.6980, 0.7569,  ..., 0.6941, 0.6824, 0.6431],\n",
       "         [0.6392, 0.6118, 0.7020,  ..., 0.6471, 0.6745, 0.6706],\n",
       "         [0.6196, 0.5961, 0.6824,  ..., 0.6157, 0.6314, 0.6431],\n",
       "         ...,\n",
       "         [0.2275, 0.2510, 0.2510,  ..., 0.2510, 0.2510, 0.2510],\n",
       "         [0.2275, 0.2510, 0.2510,  ..., 0.2510, 0.2510, 0.2510],\n",
       "         [0.1412, 0.1569, 0.1569,  ..., 0.1569, 0.1608, 0.1569]],\n",
       " \n",
       "        [[0.5882, 0.6157, 0.6745,  ..., 0.7059, 0.6941, 0.6549],\n",
       "         [0.5647, 0.5373, 0.6235,  ..., 0.6588, 0.6863, 0.6824],\n",
       "         [0.5608, 0.5333, 0.6196,  ..., 0.6275, 0.6431, 0.6549],\n",
       "         ...,\n",
       "         [0.2039, 0.2275, 0.2275,  ..., 0.2275, 0.2275, 0.2275],\n",
       "         [0.2078, 0.2353, 0.2353,  ..., 0.2353, 0.2353, 0.2353],\n",
       "         [0.1333, 0.1451, 0.1451,  ..., 0.1451, 0.1451, 0.1451]]], )}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
